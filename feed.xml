<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ltinphan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ltinphan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-19T23:35:41+00:00</updated><id>https://ltinphan.github.io/feed.xml</id><title type="html">Tin Robotics</title><subtitle>A Robotics PhD crafting legged robots, autopilots for autonomous vehicles, and fleet management systems for coordinating mobile robots in warehouses and factories. This is my space to share research, projects, and ideas. </subtitle><entry><title type="html">Run Huggingface Models Locally For Free</title><link href="https://ltinphan.github.io/blog/2025/huggingface-local/" rel="alternate" type="text/html" title="Run Huggingface Models Locally For Free"/><published>2025-06-15T15:09:00+00:00</published><updated>2025-06-15T15:09:00+00:00</updated><id>https://ltinphan.github.io/blog/2025/huggingface-local</id><content type="html" xml:base="https://ltinphan.github.io/blog/2025/huggingface-local/"><![CDATA[<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="gh"># Hugging Face Explained: Run AI Models Locally For Free</span>

If you've been following AI advancements in the open-source space, you've likely heard of Hugging Face. This French-American company is dedicated to open-source machine learning tools, particularly focusing on natural language processing. Hugging Face is named after a cute emoji.

<span class="gu">## Hugging Face's Core Offerings</span>

Hugging Face provides several key tools to empower developers and researchers in the AI community:
<span class="p">*</span>   <span class="gs">**Transformers Library**</span>: This is a collection of pre-trained models designed for various tasks such as text classification, translation, summarization, and more.
<span class="p">*</span>   <span class="gs">**Datasets Library**</span>: A repository offering ready-to-use datasets for training and evaluating machine learning models.
<span class="p">*</span>   <span class="gs">**Hugging Face Hub**</span>: A central platform that functions like the GitHub of AI, allowing users to share and explore models, datasets, and machine learning applications. It facilitates collaboration among machine learning enthusiasts and experts, enabling them to learn from each other's work and experience.

<span class="gu">## Understanding Key Hugging Face Terminologies</span>

To maximize your experience with Hugging Face, it's helpful to understand some core terms:

<span class="gu">### Pre-trained Model</span>
A pre-trained model is a machine learning model that has already been taught using a large amount of data for a specific task, such as recognizing images or understanding language. This means you can use it immediately without needing to train it from scratch.

<span class="gu">### Inference</span>
Inference is the process where a trained model makes predictions or draws conclusions about new, previously unseen data, based on patterns it learned during its training phase.

<span class="gu">### Training</span>
Training is the initial phase of the machine learning process where the machine learns by being exposed to numerous examples. For instance, if you're teaching it to recognize cars, you provide it with many labeled pictures of different cars, allowing the machine to build its understanding. Once trained, the machine uses this acquired knowledge during inference to recognize new things, like a car it hasn't seen before.

<span class="gu">### Transformers</span>
Transformers are a type of model specifically designed to handle text-based tasks, including translation, summarization, and text generation. Their unique architecture uses "attention mechanisms" to identify and capture the relationships between words and sentences.

<span class="gu">### Tokenizer</span>
A tokenizer is a process that breaks down text into smaller units known as tokens. These tokens are typically words or subwords and are essential for natural language processing tasks.

<span class="gu">## Getting Started with Hugging Face</span>

To begin using Hugging Face, you'll need to set up an account and install the necessary libraries and dependencies.

<span class="gu">### Account Setup</span>
Signing up as a community individual contributor is free. You can create a free account by visiting the Hugging Face website and clicking "sign up". You may need to complete a quick human verification, then enter your email and password, complete your profile, and generate an avatar. After creating the account, you'll receive a verification link in your email that you must click to verify your address. Once verified, you are officially a Hugging Face member. For more features or organizational needs, pro or customized plans are available.

<span class="gu">### Environment Setup: Python, Pip, and Virtual Environments</span>
Before using the Hugging Face Hub programmatically, you must set up your environment:
<span class="p">1.</span>  <span class="gs">**Install Python and Pip**</span>: Ensure Python 3.8 or higher is installed. Pip, Python's package manager, is included with Python and doesn't need separate installation.
<span class="p">2.</span>  <span class="gs">**Create a Virtual Environment**</span>: It's recommended to create a virtual environment to isolate project dependencies from system-wide packages, leading to a cleaner and more manageable setup. Common options include <span class="sb">`Venv`</span> (included with Python 3.3+) and <span class="sb">`Conda`</span> (installed via Anaconda or Miniconda), which is popular for data science and ML projects. You can activate your environment using the <span class="sb">`conda activate`</span> command.
<span class="p">3.</span>  <span class="gs">**Choose a Code Editor/IDE**</span>: For development, you can use any preferred code editor or IDE, such as Jupyter Notebook, PyCharm, or Visual Studio Code.

<span class="gu">### Installing Hugging Face Libraries</span>
Hugging Face offers two main libraries for accessing pre-trained models:
<span class="p">*</span>   <span class="gs">**Transformers**</span>: This library handles text-based tasks like translation, summarization, and text generation. Install it using <span class="sb">`pip install transformers`</span>.
<span class="p">*</span>   <span class="gs">**Diffusers**</span>: This library manages image-based tasks, including image synthesis, image editing, and image captioning.

To enable the <span class="sb">`pipeline`</span> function (discussed next) to load models, you'll also need a model backend like PyTorch or TensorFlow. To install TensorFlow, run <span class="sb">`pip install TensorFlow`</span>.

<span class="gu">## Using Pre-trained Models with Hugging Face</span>

<span class="gu">### The Pipeline Function</span>
The <span class="sb">`pipeline`</span> function, imported from the Hugging Face Transformers library, is a high-level helper function that simplifies the use of pre-trained models for common tasks. It automates processes like loading the appropriate model, tokenizing input, running the model, and formatting output, requiring only a few lines of code.

<span class="gu">### Example: Sentiment Analysis with DistilBERT</span>
Hereâ€™s how you can use the <span class="sb">`pipeline`</span> function for sentiment analysis:
<span class="p">1.</span>  <span class="gs">**Import the pipeline**</span>: <span class="sb">`from transformers import pipeline`</span>.
<span class="p">2.</span>  <span class="gs">**Load the pre-trained model**</span>: Use <span class="sb">`pipeline`</span> with the <span class="sb">`sentiment-analysis`</span> parameter and specify a model like <span class="sb">`"distilbert-base-uncased-finetuned-sst2-english"`</span>.
<span class="p">    *</span>   <span class="sb">`DistilBERT`</span> is a smaller, faster version of the BERT model.
<span class="p">    *</span>   <span class="sb">`base uncased`</span> means it processes lowercase text and disregards capitalization.
<span class="p">    *</span>   <span class="sb">`fine-tuned on SST2`</span> indicates it was specifically trained for sentiment classification using the SST2 dataset.
    This setup creates a ready-to-use tool to determine if a sentence's sentiment is positive or negative.
<span class="p">3.</span>  <span class="gs">**Analyze text**</span>: Store your desired text in a variable (e.g., <span class="sb">`inputtext`</span>), then call the <span class="sb">`pipeline`</span> with this variable and print the output.
<span class="p">4.</span>  <span class="gs">**Run the code**</span>: Ensure the correct Conda environment is active. If using VS Code, select the appropriate Python interpreter via the command palette (Shift+Command+P). Execute the Python file from the terminal (e.g., <span class="sb">`python your_file_name.py`</span>).

The model will return the sentiment analysis. For example, it identified one text as negative with 99.96% confidence and another positive sentence confidently as positive. The process demonstrates how straightforward it is to get a pre-trained model running using Hugging Face libraries. A dependency conflict (Keras) was resolved by installing the recommended version during one execution.

<span class="gu">## Discovering and Exploring Models</span>

Finding the right pre-trained model for a specific task is simple on the Hugging Face website. You can browse models and filter them by task, library, language, and other criteria. Models and datasets can also be searched by keyword and sorted by trending, most likes, most downloads, or recent updates.

Every model on the Hugging Face Hub has a "model card" containing important information such as model details, usage examples, links to files, and community interaction features. You can also view "Spaces" that use a particular model and explore them further or even clone a Space to your local machine to run it yourself.

<span class="gu">## Conclusion</span>

You are now equipped with the essential tools and knowledge to start integrating Hugging Face models into your own AI applications.

</code></pre></div></div>]]></content><author><name></name></author><category term="external-services"/><category term="AI"/><category term="code"/><summary type="html"><![CDATA[a step-to-step guideline to run huggingface models locally]]></summary></entry></feed>